"""
Web Parser - –ü–∞—Ä—Å–µ—Ä –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü —Å BeautifulSoup
"""

import logging
import aiohttp
from bs4 import BeautifulSoup
from typing import List
import sys
from pathlib import Path

# Add parent to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

from core.interfaces.parser_interface import (
    ParserInterface,
    ParserType,
    ParsedItem,
    ParserConfig
)

logger = logging.getLogger(__name__)

class WebParser(ParserInterface):
    """–ü–∞—Ä—Å–µ—Ä HTML —Å—Ç—Ä–∞–Ω–∏—Ü"""
    
    @property
    def parser_type(self) -> ParserType:
        return ParserType.WEB
    
    @property
    def parser_name(self) -> str:
        return "web_parser"
    
    @property
    def parser_version(self) -> str:
        return "1.0.0"
    
    @property
    def parser_description(self) -> str:
        return "üñ§Ô∏è HTML web page parser using BeautifulSoup"
    
    async def initialize(self, config: ParserConfig) -> None:
        """–û–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è"""
        self.config = config
        self.css_selectors = config.get('css_selectors', {})
        logger.info(f"‚úÖ {self.parser_name} initialized")
    
    async def parse(self, source: str) -> List[ParsedItem]:
        """–ü–∞—Ä—Å–∏—Ç—å HTML —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        items = []
        
        try:
            logger.info(f"üñ§Ô∏è Parsing web page from {source}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º HTML
            html = await self._fetch_url(source)
            soup = BeautifulSoup(html, 'html.parser')
            
            # –ü–∞—Ä—Å–∏–º —ç–ª–µ–º–µ–Ω—Ç—ã
            article_selector = self.css_selectors.get('articles', 'article')
            article_elements = soup.select(article_selector)[:10]
            
            for article in article_elements:
                try:
                    item = ParsedItem()
                    
                    # –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–æ–ª—è
                    title_selector = self.css_selectors.get('title', 'h1, h2, .title')
                    title_elem = article.select_one(title_selector)
                    item['title'] = title_elem.get_text(strip=True) if title_elem else 'No title'
                    
                    content_selector = self.css_selectors.get('content', 'p, .content, .description')
                    content_elem = article.select_one(content_selector)
                    item['content'] = content_elem.get_text(strip=True) if content_elem else 'No content'
                    
                    link_elem = article.find('a', href=True)
                    item['url'] = link_elem['href'] if link_elem else ''
                    
                    item['source'] = source
                    item['type'] = 'web_item'
                    
                    items.append(item)
                
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error parsing article: {e}")
                    continue
            
            logger.info(f"‚úÖ Parsed {len(items)} items from web page")
        
        except Exception as e:
            logger.error(f"‚ùå Error parsing web page: {e}")
            raise
        
        return items
    
    async def validate_source(self, source: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å URL"""
        return source.startswith('http')
    
    async def test_connection(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ"""
        return True
    
    async def _fetch_url(self, url: str) -> str:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å HTML"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    url,
                    timeout=aiohttp.ClientTimeout(total=15),
                    headers={
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    }
                ) as resp:
                    if resp.status == 200:
                        return await resp.text()
                    else:
                        raise Exception(f"HTTP {resp.status}")
        except Exception as e:
            logger.error(f"‚ùå Failed to fetch {url}: {e}")
            raise
    
    def get_config_schema(self):
        """–ö–æ–Ω—Ñ–∏–≥ —Å—Ö–µ–º–∞"""
        return {
            "type": "object",
            "properties": {
                "url": {
                    "type": "string",
                    "description": "URL –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã"
                },
                "css_selectors": {
                    "type": "object",
                    "description": "CSS —Å–µ–ª–µ–∫—Ç–æ—Ä—ã",
                    "properties": {
                        "articles": {"type": "string"},
                        "title": {"type": "string"},
                        "content": {"type": "string"}
                    }
                }
            },
            "required": ["url"]
        }
